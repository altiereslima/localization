{
  "noInstanceSelected": "未選擇模型執行個體",
  "resetToDefault": "重設",
  "showAdvancedSettings": "顯示進階設定",
  "showAll": "全部",
  "basicSettings": "基本設定",
  "configSubtitle": "載入或儲存預設值，並實驗模型參數覆寫",
  "inferenceParameters/title": "預測參數",
  "inferenceParameters/info": "實驗影響預測的參數。",
  "generalParameters/title": "一般",
  "samplingParameters/title": "取樣",
  "basicTab": "基本",
  "advancedTab": "進階",
  "advancedTab/title": "🧪 進階設定",
  "advancedTab/expandAll": "展開全部",
  "advancedTab/overridesTitle": "設定覆寫",
  "advancedTab/noConfigsText": "您沒有未儲存的變更 - 編輯上方的值以在此處查看覆寫。",
  "loadInstanceFirst": "載入模型以檢視可配置參數",
  "noListedConfigs": "沒有可配置的參數",
  "generationParameters/info": "實驗影響文字產生的基本參數。",
  "loadParameters/title": "載入參數",
  "loadParameters/description": "控制模型初始化和載入到記憶體中的設定。",
  "loadParameters/reload": "重新載入以套用變更",
  "loadParameters/reload/error": "模型重新載入失敗",
  "discardChanges": "捨棄變更",
  "loadModelToSeeOptions": "載入模型以查看選項",
  "schematicsError.title": "設定結構在以下欄位中包含錯誤：",
  "manifestSections": {
    "structuredOutput/title": "結構化輸出",
    "speculativeDecoding/title": "推測式解碼",
    "sampling/title": "取樣",
    "settings/title": "設定",
    "toolUse/title": "工具使用",
    "promptTemplate/title": "提示範本",
    "customFields/title": "自訂欄位"
  },

  "llm.prediction.systemPrompt/title": "系統提示詞",
  "llm.prediction.systemPrompt/description": "使用此欄位為模型提供背景指示，例如規則集、限制或一般要求。",
  "llm.prediction.systemPrompt/subTitle": "AI 指導方針",
  "llm.prediction.systemPrompt/openEditor": "編輯器",
  "llm.prediction.systemPrompt/closeEditor": "關閉編輯器",
  "llm.prediction.systemPrompt/openedEditor": "在編輯器中開啟...",
  "llm.prediction.temperature/title": "溫度",
  "llm.prediction.temperature/subTitle": "引入多少隨機性。 0 將每次產生相同的結果，而較高的值將增加創造力和變異。",
  "llm.prediction.temperature/info": "來自 llama.cpp 的說明文件：「預設值為 <{{dynamicValue}}>, 這在隨機性和確定性之間取得了平衡。極端情況下，溫度為 0 時將始終選擇最有可能的下一個 token，導致每次執行時產生相同的輸出」",
  "llm.prediction.llama.sampling/title": "取樣",
  "llm.prediction.topKSampling/title": "Top K 取樣",
  "llm.prediction.topKSampling/subTitle": "將下一個 token 限制為最有可能的 top-k 個 token 中的其中之一。作用類似於溫度。",
  "llm.prediction.topKSampling/info": "來自 llama.cpp 的說明文件：\n\nTop-k 取樣是一種文字產生方法，僅從模型預測的最可能的前 k 個 token 中選擇下一個 token。\n\n它有助於降低生成低概率或無意義的 token 的風險，但也可能會限制輸出的多樣性。\n\n較高的 top-k 值（例如 100）將考慮更多 token 並產生更多樣化的文字，而較低的 top-k 值（例如 10）將專注於最可能的 token 並生成更保守的文字。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.llama.cpuThreads/title": "CPU 執行緒",
  "llm.prediction.llama.cpuThreads/subTitle": "在推論期間使用的 CPU 執行緒數量",
  "llm.prediction.llama.cpuThreads/info": "計算期間使用的執行緒數量。增加執行緒數量並不總是與更好的效能相關。預設值為 <{{dynamicValue}}>。",
  "llm.prediction.maxPredictedTokens/title": "限制回應長度",
  "llm.prediction.maxPredictedTokens/subTitle": "可選地限制 AI 回應的長度",
  "llm.prediction.maxPredictedTokens/info": "控制聊天機器人回應的最大長度。開啟以設定回應最大長度的限制，或關閉以讓聊天機器人決定何時停止。",
  "llm.prediction.maxPredictedTokens/inputLabel": "最大回應長度（token）",
  "llm.prediction.maxPredictedTokens/wordEstimate": "約 {{maxWords}} 個單字",
  "llm.prediction.repeatPenalty/title": "重複懲罰",
  "llm.prediction.repeatPenalty/subTitle": "如何減少重複相同的 token",
  "llm.prediction.repeatPenalty/info": "來自 llama.cpp 的說明文件：「有助於防止模型產生重複或單調的文字。\n\n較高的值（例如 1.5）將更強烈地懲罰重複，而較低的值（例如 0.9）則會更寬容。」• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.minPSampling/title": "Min P 取樣",
  "llm.prediction.minPSampling/subTitle": "用於選擇輸出 token 的最小基於概率",
  "llm.prediction.minPSampling/info": "來自 llama.cpp 的說明文件：\n\n要考慮的 token 的最小概率，與最可能 token 的概率相對。必須在 [0, 1] 範圍內。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.topPSampling/title": "Top P 取樣",
  "llm.prediction.topPSampling/subTitle": "可能下一個 token 的最小累積概率。作用類似於溫度。",
  "llm.prediction.topPSampling/info": "來自 llama.cpp 的說明文件：\n\nTop-p 取樣，也稱為 nucleus sampling，是另一種文字產生方法，它從具有至少 p 個累積概率的 token 子集中的下一個 token 進行選擇。\n\n此方法通過考慮 token 的概率和要採樣的 token 數量，在多樣性和質量之間取得平衡。\n\ntop-p 值越高（例如 0.95），生成的文字就越多元化，而 top-p 值較低（例如 0.5）則會生成更專注、更保守的文字。必須在 (0, 1] 範圍內。\n\n• 預設值為 <{{dynamicValue}}>",
  "llm.prediction.stopStrings/title": "停止字串",
  "llm.prediction.stopStrings/subTitle": "當遇到這些字串時，應停止模型產生更多 token 的字串",
  "llm.prediction.stopStrings/info": "當遇到這些特定的字串時，將會停止模型產生更多 token",
  "llm.prediction.stopStrings/placeholder": "輸入一個字串並按下 ⏎",
  "llm.prediction.contextOverflowPolicy/title": "上下文溢出",
  "llm.prediction.contextOverflowPolicy/subTitle": "當對話變得太大，模型無法處理時，模型的行為方式",
  "llm.prediction.contextOverflowPolicy/info": "當對話超出模型工作記憶體（「上下文」）大小時，決定要如何處理",
  "llm.prediction.llama.frequencyPenalty/title": "頻率懲罰",
  "llm.prediction.llama.presencePenalty/title": "存在懲罰",
  "llm.prediction.llama.tailFreeSampling/title": "無尾取樣",
  "llm.prediction.llama.locallyTypicalSampling/title": "局部典型取樣",
  "llm.prediction.llama.xtcProbability/title": "XTC 取樣概率",
  "llm.prediction.llama.xtcProbability/subTitle": "XTC（排除頂級選擇）採樣器將以每個生成的 token 的此概率激活。XTC 採樣可以增強創造力並減少陳詞濫調。",
  "llm.prediction.llama.xtcProbability/info": "XTC（排除頂級選擇）採樣器將以每個生成的 token 的此概率激活。XTC 採樣通常可以增強創造力並減少陳詞濫調。",
  "llm.prediction.llama.xtcThreshold/title": "XTC 取樣閾值",
  "llm.prediction.llama.xtcThreshold/subTitle": "XTC（排除頂級選擇）閾值。以 `xtc-probability` 的機率，搜尋概率介於 `xtc-threshold` 和 0.5 之間的 token，並移除所有這些 token，除了最不可能的那個。",
  "llm.prediction.llama.xtcThreshold/info": "XTC（排除頂級選擇）閾值。以 `xtc-probability` 的機率，搜尋概率介於 `xtc-threshold` 和 0.5 之間的 token，並移除所有這些 token，除了最不可能的那個。",
  "llm.prediction.mlx.topKSampling/title": "Top K 取樣",
  "llm.prediction.mlx.topKSampling/subTitle": "將下一個 token 限制在最有可能的頂 K 個 token 中。作用類似於溫度。",
  "llm.prediction.mlx.topKSampling/info": "將下一個 token 限制在最有可能的頂 K 個 token 中。作用類似於溫度。",
  "llm.prediction.onnx.topKSampling/title": "Top K 取樣",
  "llm.prediction.onnx.topKSampling/subTitle": "將下一個 token 限制在最有可能的頂 K 個 token 中。作用類似於溫度。",
  "llm.prediction.onnx.topKSampling/info": "來自 ONNX 文件：\n\n保留用於頂 K 過濾器的最高概率詞彙表 token 的數量。\n\n• 此過濾器預設關閉。",
  "llm.prediction.onnx.repeatPenalty/title": "重複懲罰",
  "llm.prediction.onnx.repeatPenalty/subTitle": "如何減少重複相同的 token",
  "llm.prediction.onnx.repeatPenalty/info": "較高的值會阻止模型重複自身。",
  "llm.prediction.onnx.topPSampling/title": "Top P 取樣",
  "llm.prediction.onnx.topPSampling/subTitle": "可能下一個 token 的最小累積概率。作用類似於溫度。",
  "llm.prediction.onnx.topPSampling/info": "來自 ONNX 文件：\n\n僅保留概率加總到 TopP 或更高的最有可能的 token 以進行生成。\n\n• 此過濾器預設關閉。",
  "llm.prediction.seed/title": "種子",
  "llm.prediction.structured/title": "結構化輸出",
  "llm.prediction.structured/info": "結構化輸出",
  "llm.prediction.structured/description": "進階：您可以提供 [JSON Schema](https://json-schema.org/learn/miscellaneous-examples) 以強制模型產生特定的輸出格式。閱讀 [文件](https://lmstudio.ai/docs/advanced/structured-output) 以了解更多資訊",
  "llm.prediction.tools/title": "工具使用",
  "llm.prediction.tools/description": "進階：您可以提供符合 JSON 的工具清單，讓模型請求呼叫。閱讀 [文件](https://lmstudio.ai/docs/advanced/tool-use) 以了解更多資訊",
  "llm.prediction.tools/serverPageDescriptionAddon": "使用伺服器 API 時，將此內容作為 `tools` 傳遞至要求主體",
  "llm.prediction.promptTemplate/title": "提示範本",
  "llm.prediction.promptTemplate/subTitle": "聊天中訊息的格式，發送給模型。 更改此設定可能會導致意外行為 - 請確保您知道自己在做什麼！",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/title": "要產生的草稿 Token 數量",
  "llm.prediction.speculativeDecoding.numDraftTokensExact/subTitle": "每個主要模型 Token 要使用草稿模型產生的 Token 數量。 在計算與獎勵之間找到最佳平衡點。",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/title": "草稿概率截止值",
  "llm.prediction.speculativeDecoding.minContinueDraftingProbability/subTitle": "在 Token 的概率降至此閾值以下之前繼續草稿。 較高的值通常意味著較低的風險，較低的獎勵。",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/title": "最小草稿大小",
  "llm.prediction.speculativeDecoding.minDraftLengthToConsider/subTitle": "小於此值的草稿將由主要模型忽略。 較高的值通常意味著較低的風險，較低的獎勵。",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/title": "最大草稿大小",
  "llm.prediction.speculativeDecoding.maxTokensToDraft/subTitle": "允許在草稿中的 Token 最大數量。 如果所有 Token 的概率都 > 截止值，則為上限。 較低的值通常意味著較低的風險，較低的獎勵。",
  "llm.prediction.speculativeDecoding.draftModel/title": "草稿模型",
  "llm.prediction.reasoning.parsing/title": "推理區段解析",
  "llm.prediction.reasoning.parsing/subTitle": "如何解析模型輸出中的推理區段。",

  "llm.load.mainGpu/title": "主要 GPU",
  "llm.load.mainGpu/subTitle": "優先用於模型計算的 GPU",
  "llm.load.mainGpu/placeholder": "選擇主要 GPU...",
  "llm.load.splitStrategy/title": "分割策略",
  "llm.load.splitStrategy/subTitle": "如何將模型計算分散到多個 GPU 上",
  "llm.load.splitStrategy/placeholder": "選擇分割策略...",
  "llm.load.offloadKVCacheToGpu/title": "將 KV 緩存卸載到 GPU 記憶體",
  "llm.load.offloadKVCacheToGpu/subTitle": "將 KV 緩存卸載到 GPU 記憶體。 可以提高效能，但需要更多 GPU 記憶體。",
  "load.gpuStrictVramCap/title": "限制模型卸載到專用 GPU 記憶體",
  "load.gpuStrictVramCap.customSubTitleOff": "關閉：如果專用 GPU 記憶體已滿，允許模型權重卸載到共享記憶體。",
  "load.gpuStrictVramCap.customSubTitleOn": "開啟：系統將限制模型權重的卸載到專用 GPU 記憶體和 RAM。 上下文仍可以使用共享記憶體。",
  "load.gpuStrictVramCap.customGpuOffloadWarning": "模型卸載限制在專用 GPU 記憶體中。 實際卸載的層數可能不同。",
  "load.allGpusDisabledWarning": "所有 GPU 目前都已停用。 啟用至少一個以進行卸載。",

  "llm.load.contextLength/title": "上下文長度",
  "llm.load.contextLength/subTitle": "模型在單一提示中可以關注的最大 Token 數量。 如需更多管理此項的方法，請參閱「推論參數」下的「對話超流」選項。",
  "llm.load.contextLength/info": "指定模型一次可以考慮的最大 Token 數量，影響其在處理期間保留的上下文量。",
  "llm.load.contextLength/warning": "為上下文長度設定較高的值可能會顯著影響記憶體使用量。",
  "llm.load.seed/title": "種子",
  "llm.load.seed/subTitle": "用於文本生成中隨機數產生器的種子。 -1 為隨機。",
  "llm.load.seed/info": "隨機種子：設定隨機數生成的種子，以確保可重複的結果。",

  "llm.load.llama.evalBatchSize/title": "評估批次大小",
  "llm.load.llama.evalBatchSize/subTitle": "一次處理輸入 Token 的數量。 增加此值可以提高效能，但會增加記憶體使用量。",
  "llm.load.llama.evalBatchSize/info": "設定在評估期間一起處理的範例數量，影響速度和記憶體使用量。",
  "llm.load.llama.ropeFrequencyBase/title": "RoPE 頻率基數",
  "llm.load.llama.ropeFrequencyBase/subTitle": "旋轉位置嵌入 (RoPE) 的自訂基數頻率。 增加此值可能有助於在高上下文長度下提高效能。",
  "llm.load.llama.ropeFrequencyBase/info": "[進階] 調整旋轉位置編碼的基數頻率，影響位置資訊的嵌入方式。",
  "llm.load.llama.ropeFrequencyScale/title": "RoPE 頻率比例",
  "llm.load.llama.ropeFrequencyScale/subTitle": "使用此係數縮放上下文長度，以透過 RoPE 擴展有效上下文。",
  "llm.load.llama.ropeFrequencyScale/info": "[進階] 修改旋轉位置編碼的頻率縮放，以控制位置編碼粒度。",
  "llm.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "llm.load.llama.acceleration.offloadRatio/subTitle": "用於 GPU 加速的模型離散層數。",
  "llm.load.llama.acceleration.offloadRatio/info": "設定要卸載到 GPU 的圖層數量。",
  "llm.load.llama.flashAttention/title": "Flash Attention（高速注意力機制）",
  "llm.load.llama.flashAttention/subTitle": "在某些模型上減少記憶體使用量和生成時間。",
  "llm.load.llama.flashAttention/info": "加速注意力機制，實現更快速、更高效的處理。",
  "llm.load.numExperts/title": "專家數量",
  "llm.load.numExperts/subTitle": "模型中使用的專家數量。",
  "llm.load.numExperts/info": "模型中使用的專家數量。",
  "llm.load.llama.keepModelInMemory/title": "保留模型在記憶體中",
  "llm.load.llama.keepModelInMemory/subTitle": "即使已卸載到 GPU，也保留系統記憶體用於模型。可提高效能，但需要更多系統 RAM。",
  "llm.load.llama.keepModelInMemory/info": "防止將模型交換到磁碟，確保更快的存取速度，但會增加 RAM 使用量。",
  "llm.load.llama.useFp16ForKVCache/title": "對 KV 快取使用 FP16",
  "llm.load.llama.useFp16ForKVCache/info": "透過儲存快取於半精度 (FP16) 中，減少記憶體使用量。",
  "llm.load.llama.tryMmap/title": "嘗試 mmap()",
  "llm.load.llama.tryMmap/subTitle": "可改善模型載入時間。停用此功能可能在模型大於可用系統 RAM 時提高效能。",
  "llm.load.llama.tryMmap/info": "直接從磁碟將模型檔案載入到記憶體中。",
  "llm.load.llama.cpuThreadPoolSize/title": "CPU 執行緒池大小",
  "llm.load.llama.cpuThreadPoolSize/subTitle": "為用於模型計算的執行緒池分配的 CPU 執行緒數量。",
  "llm.load.llama.cpuThreadPoolSize/info": "為用於模型計算的執行緒池分配的 CPU 執行緒數量。增加執行緒數量並不總是與更好的效能相關。預設值為 <{{dynamicValue}}>。",
  "llm.load.llama.kCacheQuantizationType/title": "K 快取量化類型",
  "llm.load.llama.kCacheQuantizationType/subTitle": "較低的值可減少記憶體使用量，但可能降低品質。效果因模型而異。",
  "llm.load.llama.vCacheQuantizationType/title": "V 快取量化類型",
  "llm.load.llama.vCacheQuantizationType/subTitle": "較低的值可減少記憶體使用量，但可能降低品質。效果因模型而異。",
  "llm.load.llama.vCacheQuantizationType/turnedOnWarning": "⚠️ 如果未啟用 Flash Attention，則必須停用此值。",
  "llm.load.llama.vCacheQuantizationType/disabledMessage": "只有在啟用 Flash Attention 時才能開啟。",
  "llm.load.llama.vCacheQuantizationType/invalidF32MetalState": "⚠️ 使用 F32 時，必須停用 flash attention。",
  "llm.load.mlx.kvCacheBits/title": "KV 快取量化",
  "llm.load.mlx.kvCacheBits/subTitle": "KV 快取應量化的位元數量。",
  "llm.load.mlx.kvCacheBits/info": "KV 快取應量化的位元數量。",
  "llm.load.mlx.kvCacheBits/turnedOnWarning": "使用 KV 快取量化時，會忽略上下文長度設定。",
  "llm.load.mlx.kvCacheGroupSize/title": "KV 快取量化：群組大小",
  "llm.load.mlx.kvCacheGroupSize/subTitle": "在 KV 快取的量化操作期間使用的群組大小。較大的群組大小可減少記憶體使用量，但可能降低品質。",
  "llm.load.mlx.kvCacheGroupSize/info": "KV 快取應量化的位元數量。",
  "llm.load.mlx.kvCacheQuantizationStart/title": "KV 快取量化：當 ctx 跨越此長度時開始量化",
  "llm.load.mlx.kvCacheQuantizationStart/subTitle": "開始量化 KV 快取的上下文長度閾值。",
  "llm.load.mlx.kvCacheQuantizationStart/info": "開始量化 KV 快取的上下文長度閾值。",
  "llm.load.mlx.kvCacheQuantization/title": "KV 快取量化",
  "llm.load.mlx.kvCacheQuantization/subTitle": "量化模型的 KV 快取。這可能會導致更快的生成速度和更低的記憶體佔用空間，但會以犧牲模型輸出的品質為代價。",
  "llm.load.mlx.kvCacheQuantization/bits/title": "KV 快取量化的位元",
  "llm.load.mlx.kvCacheQuantization/bits/tooltip": "要量化 KV 快取的位元數量。",
  "llm.load.mlx.kvCacheQuantization/bits/bits": "位元",
  "llm.load.mlx.kvCacheQuantization/groupSize/title": "群組大小策略",
  "llm.load.mlx.kvCacheQuantization/groupSize/accuracy": "精確度",
  "llm.load.mlx.kvCacheQuantization/groupSize/balanced": "平衡",
  "llm.load.mlx.kvCacheQuantization/groupSize/speedy": "快速",
  "llm.load.mlx.kvCacheQuantization/groupSize/tooltip": "進階：量化的 'matmul' 群組大小配置\n\n• 精確度 = 群組大小 32\n• 平衡 = 群組大小 64\n• 快速 = 群組大小 128\n",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/title": "當 ctx 達到此長度時開始量化",
  "llm.load.mlx.kvCacheQuantization/quantizedStart/tooltip": "當上下文達到此數量的 token 時，\n開始量化 KV 快取。",

  "embedding.load.contextLength/title": "上下文長度",
  "embedding.load.contextLength/subTitle": "模型在一個提示中可以關注的最大 token 數量。請參閱「推論參數」下的「對話溢出」選項，以瞭解更多管理此項的方法。",
  "embedding.load.contextLength/info": "指定模型一次可以考慮的最大 token 數量，影響其在處理期間保留的上下文量。",
  "embedding.load.llama.ropeFrequencyBase/title": "RoPE 頻率基底",
  "embedding.load.llama.ropeFrequencyBase/subTitle": "旋轉位置嵌入 (RoPE) 的自訂基底頻率。增加此值可能可在高上下文長度下實現更好的效能。",
  "embedding.load.llama.ropeFrequencyBase/info": "[進階] 調整旋轉位置編碼的基底頻率，影響如何嵌入位置資訊。",
  "embedding.load.llama.evalBatchSize/title": "評估批次大小",
  "embedding.load.llama.evalBatchSize/subTitle": "一次處理的輸入 token 數量。增加此值可在效能和記憶體使用量之間取得平衡。",
  "embedding.load.llama.evalBatchSize/info": "設定在評估期間一起處理的 token 數量。",
  "embedding.load.llama.ropeFrequencyScale/title": "RoPE 頻率比例",
  "embedding.load.llama.ropeFrequencyScale/subTitle": "上下文長度會根據此係數進行縮放，以使用 RoPE 擴展有效上下文。",
  "embedding.load.llama.ropeFrequencyScale/info": "[進階] 修改旋轉位置編碼的頻率比例，以控制位置編碼粒度。",
  "embedding.load.llama.acceleration.offloadRatio/title": "GPU 卸載",
  "embedding.load.llama.acceleration.offloadRatio/subTitle": "用於 GPU 加速的 GPU 上計算的離散模型層數量。",
  "embedding.load.llama.acceleration.offloadRatio/info": "設定要卸載到 GPU 的層數。",
  "embedding.load.llama.keepModelInMemory/title": "保留模型在記憶體中",
  "embedding.load.llama.keepModelInMemory/subTitle": "即使已卸載到 GPU，也要為模型保留系統記憶體。可提高效能，但需要更多系統 RAM。",
  "embedding.load.llama.keepModelInMemory/info": "防止模型被交換到磁碟，確保更快的存取速度，但會增加 RAM 使用量。",
  "embedding.load.llama.tryMmap/title": "嘗試 mmap()",
  "embedding.load.llama.tryMmap/subTitle": "可改善模型的載入時間。停用此功能可能在模型大於可用系統 RAM 時提高效能。",
  "embedding.load.llama.tryMmap/info": "直接從磁碟將模型檔案載入到記憶體中。",
  "embedding.load.seed/title": "種子",
  "embedding.load.seed/subTitle": "用於文字生成的隨機數生成器的種子。-1 為隨機種子",

  "embedding.load.seed/info": "隨機種子：設定隨機數生成器的種子，以確保可重複的結果。",

  "presetTooltip": {
    "included/title": "預設值",
    "included/description": "將應用下列欄位",
    "included/empty": "此上下文環境中沒有任何預設值的欄位適用。",
    "included/conflict": "您會被要求選擇是否要套用此值",
    "separateLoad/title": "載入時設定",
    "separateLoad/description.1": "這個預設值也包含下列載入時的配置。載入時的配置是模型範圍內的，需要重新載入模型才能生效。按住",
    "separateLoad/description.2": "以套用至",
    "separateLoad/description.3": "。",
    "excluded/title": "可能不適用",
    "excluded/description": "下列欄位包含在預設值中，但目前環境下不適用。",
    "legacy/title": "舊版預設值",
    "legacy/description": "這個預設值是舊版的預設值。它包含以下欄位，這些欄位現在已自動處理或不再適用。",
    "button/publish": "發布到 Hub",
    "button/pushUpdate": "將變更推送到 Hub",
    "button/noChangesToPush": "沒有要推送的變更",
    "button/export": "匯出",
    "hubLabel": "來自 Hub 的預設值，作者：{{user}}",
    "ownHubLabel": "從 Hub 匯出的預設值"
  },

  "customInputs": {
    "string": {
      "emptyParagraph": "<空的>"
    },
    "checkboxNumeric": {
      "off": "關閉"
    },
    "llamaCacheQuantizationType": {
      "off": "關閉"
    },
    "mlxKvCacheBits": {
      "off": "關閉"
    },
    "stringArray": {
      "empty": "<空的>"
    },
    "llmPromptTemplate": {
      "type": "類型",
      "types.jinja/label": "範本 (Jinja)",
      "jinja.bosToken/label": "BOS Token",
      "jinja.eosToken/label": "EOS Token",
      "jinja.template/label": "範本",
      "jinja/error": "解析 Jinja 範本失敗：{{error}}",
      "jinja/empty": "請在上方輸入 Jinja 範本。",
      "jinja/unlikelyToWork": "您提供的 Jinja 範本可能無法正常運作，因為它沒有參考變數「messages」。請仔細檢查您是否已輸入正確的範本。",
      "types.manual/label": "手動",
      "manual.subfield.beforeSystem/label": "系統前",
      "manual.subfield.beforeSystem/placeholder": "輸入系統前綴...",
      "manual.subfield.afterSystem/label": "系統後",
      "manual.subfield.afterSystem/placeholder": "輸入系統後綴...",
      "manual.subfield.beforeUser/label": "使用者前",
      "manual.subfield.beforeUser/placeholder": "輸入使用者前綴...",
      "manual.subfield.afterUser/label": "使用者後",
      "manual.subfield.afterUser/placeholder": "輸入使用者後綴...",
      "manual.subfield.beforeAssistant/label": "助理前",
      "manual.subfield.beforeAssistant/placeholder": "輸入助理前綴...",
      "manual.subfield.afterAssistant/label": "助理後",
      "manual.subfield.afterAssistant/placeholder": "輸入助理後綴...",
      "stopStrings/label": "額外的停止字串",
      "stopStrings/subTitle": "範本特定的停止字串，將除了使用者指定的停止字串之外使用。"
    },
    "contextLength": {
      "maxValueTooltip": "這是模型訓練時可以處理的最大 Token 數量。點擊以將上下文設定為此值。",
      "maxValueTextStart": "模型支援最高",
      "maxValueTextEnd": "個 Token",
      "tooltipHint": "雖然模型可能支援到一定的 Token 數量，但如果您的機器資源無法負擔負荷，效能可能會下降 - 在增加此值時請小心使用。"
    },
    "contextOverflowPolicy": {
      "stopAtLimit": "在限制處停止",
      "stopAtLimitSub": "當模型的記憶體已滿時停止生成。",
      "truncateMiddle": "截斷中間",
      "truncateMiddleSub": "移除對話中訊息以騰出空間給較新的訊息。模型仍會記住對話的開頭。",
      "rollingWindow": "滾動視窗",
      "rollingWindowSub": "模型將始終獲得最近幾則訊息，但可能會忘記對話的開頭。"
    },
    "llamaAccelerationOffloadRatio": {
      "max": "最大值",
      "off": "關閉"
    },
    "gpuSplitStrategy": {
      "evenly": "Evenly",
      "favorMainGpu": "Favor Main GPU"
    },
    "speculativeDecodingDraftModel": {
      "readMore": "閱讀運作方式",
      "placeholder": "選擇相容的草稿模型",
      "noCompatible": "找不到與您目前的模型選擇相容的草稿模型。",
      "stillLoading": "正在識別相容的草稿模型...",
      "notCompatible": "所選的草稿模型 (<draft/>) 與目前的模型選擇 (<current/>) 不相容。",
      "off": "關閉",
      "loadModelToSeeOptions": "載入模型 <keyboard-shortcut /> 以查看相容選項",
      "compatibleWithNumberOfModels": "建議至少適用於您的 {{dynamicValue}} 個模型",
      "recommendedForSomeModels": "建議用於部分模型",
      "recommendedForLlamaModels": "建議用於 Llama 模型",
      "recommendedForQwenModels": "建議用於 Qwen 模型",
      "onboardingModal": {
        "introducing": "介紹",
        "speculativeDecoding": "推測式解碼",
        "firstStepBody": "針對 <custom-span>llama.cpp</custom-span> 和 <custom-span>MLX</custom-span> 模型的推理加速",
        "secondStepTitle": "使用推測式解碼的推理加速",
        "secondStepBody": "推測式解碼是一種涉及兩個模型協作的技術：\n - 一個較大的「主要」模型\n - 一個較小的「草稿」模型\n\n在生成過程中，草稿模型快速地提出 Token 給更大的主要模型驗證。驗證 Token 的過程比實際生成 Token 快得多，這是速度提升的來源。通常，主要模型和草稿模型之間的尺寸差異越大，加速效果越顯著。\n\n為了維持品質，主要模型僅接受與其本身會生成的 Token 相符的 Token，從而實現了較大模型的響應品質，同時具有更快的推理速度。兩個模型必須共享相同的詞彙表。",
        "draftModelRecommendationsTitle": "草稿模型建議",
        "basedOnCurrentModels": "根據您目前的模型",
        "close": "關閉",
        "next": "下一步",
        "done": "完成"
      },
      "speculativeDecodingLoadModelToSeeOptions": "請先載入模型 <model-badge /> ",
      "errorEngineNotSupported": "推測式解碼需要引擎 {{engineName}} 的至少 {{minVersion}} 版本。請更新引擎 (<key/>) 並重新載入模型以使用此功能。",
      "errorEngineNotSupported/noKey": "推測式解碼需要引擎 {{engineName}} 的至少 {{minVersion}} 版本。請更新引擎並重新載入模型以使用此功能。"
    },
    "llmReasoningParsing": {
      "startString/label": "起始字串",
      "startString/placeholder": "輸入起始字串...",
      "endString/label": "結束字串",
      "endString/placeholder": "輸入結束字串..."
    }
  },
  "saveConflictResolution": {
    "title": "選擇要包含在預設值中的數值",
    "description": "挑選您想要保留的數值",
    "instructions": "點擊一個數值以將其包含進去",
    "userValues": "先前的值",
    "presetValues": "新的值",
    "confirm": "確認",
    "cancel": "取消"
  },
  "applyConflictResolution": {
    "title": "保留哪些值？",
    "description": "您有尚未提交的變更，這些變更與傳入的預設值重疊。",
    "instructions": "點擊一個數值以保留它。",
    "userValues": "目前的值",
    "presetValues": "傳入的預設值",
    "confirm": "確認",
    "cancel": "取消"
  },
  "empty": "<空白>",
  "noModelSelected": "未選擇模型",
  "apiIdentifier.label": "API 識別碼",
  "apiIdentifier.hint": "可選地為此模型提供一個識別碼。這將用於 API 請求。留空以使用預設識別碼。",
  "idleTTL.label": "閒置時自動卸載 (TTL)",
  "idleTTL.hint": "如果設定，模型在閒置指定的時間後將會自動卸載。",
  "idleTTL.mins": "分鐘",

  "presets": {
    "title": "預設值",
    "commitChanges": "提交變更",
    "commitChanges/description": "將您的變更提交到預設值。",
    "commitChanges.manual": "偵測到新欄位。您將能夠選擇要包含在預設值中的哪些變更。",
    "commitChanges.manual.hold.0": "暫停",
    "commitChanges.manual.hold.1": "以選擇要提交到預設值的哪些變更。",
    "commitChanges.saveAll.hold.0": "暫停",
    "commitChanges.saveAll.hold.1": "以保存所有變更。",
    "commitChanges.saveInPreset.hold.0": "暫停",
    "commitChanges.saveInPreset.hold.1": "僅保存已包含在預設值中的欄位的變更。",
    "commitChanges/error": "提交到預設值的變更失敗。",
    "commitChanges.manual/description": "選擇要包含在預設值中的哪些變更。",
    "saveAs": "另存為...",
    "presetNamePlaceholder": "輸入預設值的名稱....",
    "cannotCommitChangesLegacy": "這是一個遺留的預設值，無法修改。您可以使用「另存為...」來建立副本。",
    "cannotCommitChangesNoChanges": "沒有要提交的變更。",
    "emptyNoUnsaved": "選擇一個預設值...",
    "emptyWithUnsaved": "未保存的預設值",
    "saveEmptyWithUnsaved": "另存為預設值...",
    "saveConfirm": "儲存",
    "saveCancel": "取消",
    "saving": "儲存中...",
    "save/error": "儲存預設值失敗。",
    "deselect": "取消選擇預設值",
    "deselect/error": "取消選擇預設值失敗。",
    "select/error": "選擇預設值失敗。",
    "delete/error": "刪除預設值失敗。",
    "discardChanges": "捨棄變更",
    "discardChanges/info": "捨棄所有未提交的變更，並將預設值還原為原始狀態",
    "newEmptyPreset": "+ 新增預設值",
    "importPreset": "匯入",
    "contextMenuCopyIdentifier": "複製預設值識別碼",
    "contextMenuSelect": "套用預設值",
    "contextMenuDelete": "刪除...",
    "contextMenuShare": "發布...",
    "contextMenuOpenInHub": "在網頁上檢視",
    "contextMenuPullFromHub": "取得最新版本",
    "contextMenuPushChanges": "將變更推送到 Hub",
    "contextMenuPushingChanges": "推送中...",
    "contextMenuPushedChanges": "變更已推送到",
    "contextMenuExport": "匯出檔案",
    "contextMenuRevealInExplorer": "在檔案總管中顯示",
    "contextMenuRevealInFinder": "在 Finder 中顯示",
    "share": {
      "title": "發布預設值",
      "action": "分享您的預設值，讓其他人可以下載、喜歡和複製",
      "presetOwnerLabel": "擁有者",
      "uploadAs": "您的預設值將以 {{name}} 建立",
      "presetNameLabel": "預設值名稱",
      "descriptionLabel": "描述 (可選)",
      "loading": "發布中...",
      "success": "預設值已成功推送到",
      "presetIsLive": "<preset-name /> 現在在 Hub 上線了！",
      "close": "關閉",
      "confirmViewOnWeb": "在網頁上檢視",
      "confirmCopy": "複製 URL",
      "confirmCopied": "已複製！",
      "pushedToHub": "您的預設值已推送到 Hub",
      "descriptionPlaceholder": "輸入描述...",
      "willBePublic": "這個預設值將會是公開的。網路上任何人都可以看到它。",
      "willBePrivate": "只有您才能看到這個預設值",
      "willBeOrgVisible": "這個預設值將會對組織中的每個人可見。",
      "publicSubtitle": "您的預設值是 <custom-bold>公開</custom-bold> 的。其他人可以在 lmstudio.ai 上下載和複製它",
      "privateUsageReached": "私人預設值的數量上限已達致。",
      "continueInBrowser": "在瀏覽器中繼續",
      "confirmShareButton": "發布",
      "error": "發布預設值失敗",
      "createFreeAccount": "在 Hub 中建立免費帳戶以發布預設值"
    },
    "update": {
      "title": "推動變更至 Hub",
      "title/success": "預設值已成功更新",
      "subtitle": "對 <custom-preset-name /> 進行變更並將其推送到 Hub",
      "descriptionLabel": "描述",
      "descriptionPlaceholder": "輸入描述...",
      "loading": "推送中...",
      "cancel": "取消",
      "createFreeAccount": "在 Hub 中建立免費帳戶以發布預設值",
      "error": "推動更新失敗",
      "confirmUpdateButton": "推動"
    },
    "resolve": {
      "title": "解析衝突...",
      "tooltip": "開啟一個模式對 Hub 版本差異進行解析"
    },
    "loginToManage": {
      "title": "登入以管理..."
    },
    "downloadFromHub": {
      "title": "下載",
      "downloading": "正在下載...",
      "success": "已下載！",
      "error": "下載失敗"
    },
    "push": {
      "title": "推動變更",
      "pushing": "推送中...",
      "success": "已推動",
      "tooltip": "將您的本地變更推送到 Hub 上託管的遠端版本",
      "error": "推動失敗"
    },
    "saveAsNewModal": {
      "title": "糟糕！在 Hub 上找不到這個預設值",
      "confirmSaveAsNewDescription": "您想將此預設值作為新的預設值發布嗎？",
      "confirmButton": "以新方式發布"
    },
    "pull": {
      "title": "取得最新版本",
      "error": "拉取失敗",
      "contextMenuErrorMessage": "拉取失敗",
      "success": "已拉取",
      "pulling": "正在拉取...",
      "upToDate": "已更新！",
      "unsavedChangesModal": {
        "title": "您有未儲存的變更。",
        "bodyContent": "從遠端拉取將覆寫您的未儲存的變更。繼續？",
        "confirmButton": "覆寫未儲存的變更"
      }
    },
    "import": {
      "title": "從檔案匯入預設值",
      "dragPrompt": "拖放預設值檔案 (.tar.gz 或 preset.json) 或 <custom-link>從您的電腦中選取</custom-link>",
      "remove": "移除",
      "cancel": "取消",
      "importPreset_zero": "匯入預設值",
      "importPreset_one": "匯入預設值",
      "importPreset_other": "匯入 {{count}} 個預設值",
      "selectDialog": {
        "title": "選取預設值檔案 (preset.json 或 .tar.gz)",
        "button": "匯入"
      },
      "error": "匯入預設值失敗",
      "resultsModal": {
        "titleSuccessSection_one": "成功匯入 1 個預設值",
        "titleSuccessSection_other": "成功匯入 {{count}} 個預設值",
        "titleFailSection_zero": "",
        "titleFailSection_one": "({{count}} 個失敗)",
        "titleFailSection_other": "({{count}} 個失敗)",
        "titleAllFailed": "匯入預設值失敗",
        "importMore": "匯入更多",
        "close": "完成",
        "successBadge": "成功",
        "alreadyExistsBadge": "預設值已存在",
        "errorBadge": "錯誤",
        "invalidFileBadge": "無效的檔案",
        "otherErrorBadge": "匯入預設值失敗",
        "errorViewDetailsButton": "檢視詳細資料",
        "seeError": "查看錯誤",
        "noName": "沒有預設值名稱",
        "useInChat": "在聊天中使用"
      },
      "importFromUrl": {
        "button": "從 URL 匯入 URL...",
        "title": "從 URL 匯入",
        "back": "從檔案匯入...",
        "action": "貼上您要匯入的 LM Studio Hub 預設值的 URL 下方",
        "invalidUrl": "無效的 URL。請確保您貼上的正確 LM Studio Hub URL。",
        "tip": "您可以透過 LM Studio Hub 中的 {{buttonName}} 按鈕直接安裝此預設值",
        "confirm": "匯入",
        "cancel": "取消",
        "loading": "正在匯入...",
        "error": "無法下載預設值。"
      }
    },
    "download": {
      "title": "從 LM Studio Hub 拉取 <preset-name />",
      "subtitle": "將 <custom-name /> 儲存到您的預設值。這樣您就可以在應用程式中使用這個預設值了",
      "button": "拉取",
      "button/loading": "在拉取...",
      "cancel": "取消",
      "error": "下載預設值失敗。"
    },
    "inclusiveness": {
      "speculativeDecoding": "包含在預設值中"
    }
  },

  "flashAttentionWarning": "Flash Attention 是一個實驗性功能，可能會在使用某些模型時造成問題。如果遇到問題，請嘗試停用它。",
  "llamaKvCacheQuantizationWarning": "KV 快取量化是一個實驗性功能，可能會在使用某些模型時造成問題。V 快取量化需要啟用 Flash Attention。如果遇到問題，請重設為預設值「F16」。",

  "seedUncheckedHint": "隨機種子",
  "ropeFrequencyBaseUncheckedHint": "自動",
  "ropeFrequencyScaleUncheckedHint": "自動",

  "hardware": {
    "environmentVariables": "環境變數",
    "environmentVariables.info": "如果您不確定，請將其保留在預設值。",
    "environmentVariables.reset": "重設為預設值",

    "gpus.information": "設定您機器上偵測到的圖形處理單元 (GPU)",
    "gpuSettings": {
      "editMaxCapacity": "編輯最大容量",
      "hideEditMaxCapacity": "隱藏編輯最大容量",
      "allOffWarning": "所有 GPU 都已關閉或停用，請確保有 GPU 配置才能載入模型。",
      "split": {
        "title": "策略",
        "placeholder": "選取 GPU 記憶體配置",
        "options": {
          "generalDescription": "設定如何將模型載入到您的 GPU 上",
          "evenly": {
            "title": "平均分配",
            "description": "在 GPU 上均勻分配記憶體"
          },
          "priorityOrder": {
            "title": "優先順序排序",
            "description": "拖曳以重新排序優先順序。系統會嘗試將更多資源配置到清單中列出的第一個 GPU。"
          },
          "custom": {
            "title": "自訂",
            "description": "分配記憶體",
            "maxAllocation": "最大分配量"
          }
        }
      },
      "deviceId.info": "此裝置的唯一識別碼",
      "changesOnlyAffectNewlyLoadedModels": "變更僅會影響新載入的模型",
      "toggleGpu": "啟用/停用 GPU"
    }
  },

  "load.gpuSplitConfig/title": "GPU 分割配置",
  "envVars/title": "設定環境變數",
  "envVars": {
    "select": {
      "placeholder": "選取一個環境變數...",
      "noOptions": "沒有更多選項可用",
      "filter": {
        "placeholder": "過濾搜尋結果",
        "resultsFound_zero": "找不到任何結果",
        "resultsFound_one": "找到 1 個結果",
        "resultsFound_other": "找到 {{count}} 個結果"
      }
    },
    "inputValue": {
      "placeholder": "輸入一個值"
    },
    "values": {
      "title": "目前的值"
    }
  }
}
